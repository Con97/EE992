{"cells":[{"cell_type":"code","source":["pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8efGwj186vvb","executionInfo":{"status":"ok","timestamp":1742245410290,"user_tz":0,"elapsed":4026,"user":{"displayName":"Frank Conway","userId":"18233513917736815923"}},"outputId":"3fd257b8-d819-4e07-bfc4-3576f99e7e01"},"id":"8efGwj186vvb","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-3.4.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","execution_count":17,"id":"4b508078-a659-4a44-9410-8c7c1f6a1b1d","metadata":{"id":"4b508078-a659-4a44-9410-8c7c1f6a1b1d","executionInfo":{"status":"ok","timestamp":1742247568879,"user_tz":0,"elapsed":5,"user":{"displayName":"Frank Conway","userId":"18233513917736815923"}}},"outputs":[],"source":["import tensorflow as tf\n","from transformers import TFViTForImageClassification, ViTFeatureExtractor\n","from datasets import load_dataset\n","import os\n","import datasets"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FtTlyMYSBaqu","executionInfo":{"status":"ok","timestamp":1742247161729,"user_tz":0,"elapsed":15726,"user":{"displayName":"Frank Conway","userId":"18233513917736815923"}},"outputId":"5858a0fd-a3d9-442a-d8b3-f1413749c7b6"},"id":"FtTlyMYSBaqu","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.mkdir(\"/content/drive/MyDrive/EE992/ViT/\")\n","path = \"/content/drive/MyDrive/EE992/ViT/\"\n"],"metadata":{"id":"7xojU33sBeMt","executionInfo":{"status":"ok","timestamp":1742247570631,"user_tz":0,"elapsed":7,"user":{"displayName":"Frank Conway","userId":"18233513917736815923"}}},"id":"7xojU33sBeMt","execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":4,"id":"ec4eeb6c-851a-4af0-94ee-95cf4cdcfe5f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365,"referenced_widgets":["28d48cdce69e4926886646d2e6c704b2","5c158ec86d6b4008ae170ca2967a4829","0f55b2b6bc3c4763a34db2100e2317ec","6f739c947f4b46e6bbcb1bf3da642447","b7940900916d45f59fb480b4ee16c524","dd0c6bacc163475eaf42896602590ff7","4862207b785a45b688495001752145c6","0f2b83dff1a14f1899c066d417b5c3e8","383868970b9b44de821c1c6f28f71bd2","3bbc4e7b7b9b4cef96568eeb25f9e003","eaf091a26b7d41ec9e9ccc21b485357d","ff40db26d2fe49c6abe1c112f7ed4867","7b0e000909484c3da6f9e2775e66831c","1cc3449771c540eab0e3a8f1833144ec","ae59fcd287f14348b7cbac41e8f74fdf","27e1d19e670e462c989465fa612a5a3c","027427d6290c4fc180123ccb8f170090","1a722597e3dd4382b2539cd68ca12b97","797e675107d74ed7a11eb0c1d429e48c","1fd872b209f34f4bb66480c20f3225d4","3dc8cca78e7c4f4b914cf78f757ab889","2f87ca7f64854cc19a2fd13e8920d3b2","24b07896645943c79d433b5d07ff252d","938ec031bab548e88dff2076fcbf9740","26959b84c1a64450ad97fb9fa2dcbb60","efb6a193137d425a98b653b39af902b3","1fd2d69786d441479b3de47a43125291","755f07bbfff7431bb9e69c57a90396c3","a8a9ca0e0374407c87c23b6312f42e83","75a4e280162447be837954b0aeef6750","7095f0881c604498aeb4f13b0c5bdd35","eef9813ac5c5413c95096adee1a56a6c","bd307ed83cab463d9d5930b22b00c4e3"]},"id":"ec4eeb6c-851a-4af0-94ee-95cf4cdcfe5f","executionInfo":{"status":"ok","timestamp":1742245419793,"user_tz":0,"elapsed":3937,"user":{"displayName":"Frank Conway","userId":"18233513917736815923"}},"outputId":"16826987-6fce-4ec7-d8a5-7c5181770056"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28d48cdce69e4926886646d2e6c704b2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff40db26d2fe49c6abe1c112f7ed4867"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b07896645943c79d433b5d07ff252d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFViTForImageClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model_name = \"google/vit-base-patch16-224-in21k\"\n","feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n","model = TFViTForImageClassification.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"id":"14d6d598-1d4b-43c5-9acf-fbfe7424533a","metadata":{"id":"14d6d598-1d4b-43c5-9acf-fbfe7424533a"},"outputs":[],"source":["\n","# dataset = datasets.load_dataset(\"cifar100\")\n","\n","# img_class_labels = dataset['train'].features['fine_label'].names\n","# # img_class_labels\n","# # we are also renaming our label col to labels to use `.to_tf_dataset` later\n","# dataset = dataset.rename_column(\"fine_label\", \"labels\")\n","\n","# # Create a preprocessing function\n","# def preprocess_function(examples):\n","#     images = examples[\"img\"]\n","#     inputs = feature_extractor(images=images, return_tensors=\"tf\")\n","#     inputs[\"labels\"] = examples[\"labels\"]\n","#     return inputs\n","\n","\n","# # Apply preprocessing\n","# processed_dataset = dataset.map(preprocess_function, batched=True)\n"]},{"cell_type":"code","execution_count":null,"id":"0a8c6a31-e8ed-4ca5-885b-1af6428ac0b6","metadata":{"id":"0a8c6a31-e8ed-4ca5-885b-1af6428ac0b6"},"outputs":[],"source":["# # test size will be 15% of train dataset\n","# test_size=.15\n","\n","# # Apply train_test_split to the 'train' split of the dataset\n","# processed_dataset = processed_dataset[\"train\"].train_test_split(test_size=test_size)\n","# # Update the 'train' and 'test' splits in the DatasetDict\n","# processed_dataset[\"train\"] = processed_dataset[\"train\"]\n","# processed_dataset[\"test\"] = processed_dataset[\"test\"]\n","# processed_dataset\n","\n","\n","# test_size = 0.15\n","# train_val_split = processed_dataset[\"train\"].train_test_split(test_size=test_size)\n","\n","# # Create the final dataset dictionary\n","# final_dataset = datasets.DatasetDict({\n","#     \"train\": train_val_split[\"train\"],\n","#     \"validation\": train_val_split[\"test\"],\n","#     \"test\": train_val_split[\"test\"]\n","# })\n","\n","# # Convert to TensorFlow datasets\n","# tf_train_dataset = processed_dataset[\"train\"].to_tf_dataset(\n","#     columns=[\"pixel_values\", \"labels\"],\n","#     shuffle=True,\n","#     batch_size=16,\n","#     collate_fn=None,\n","# )\n","\n","# tf_validation_dataset = processed_dataset[\"test\"].to_tf_dataset(\n","#     columns=[\"pixel_values\", \"labels\"],\n","#     shuffle=False,\n","#     batch_size=16,\n","#     collate_fn=None,\n","# )\n","\n","# tf_test_dataset = processed_dataset[\"test\"].to_tf_dataset(\n","#     columns=[\"pixel_values\", \"labels\"],\n","#     shuffle=False,\n","#     batch_size=16,\n","#     collate_fn=None,\n","# )"]},{"cell_type":"code","execution_count":null,"id":"abe1bc8d-f6fc-4009-8c3f-862c2d26669d","metadata":{"id":"abe1bc8d-f6fc-4009-8c3f-862c2d26669d"},"outputs":[],"source":["# tf_train_dataset = processed_dataset[\"train\"].to_tf_dataset(\n","#     columns=[\"pixel_values\", \"labels\"],\n","#     shuffle=True,\n","#     batch_size=16,\n","#     collate_fn=None,\n","# )\n","\n","# tf_validation_dataset = processed_dataset[\"test\"].to_tf_dataset(\n","#     columns=[\"pixel_values\", \"labels\"],\n","#     shuffle=False,\n","#     batch_size=16,\n","#     collate_fn=None,\n","# )"]},{"cell_type":"code","execution_count":9,"id":"87ca000c-9188-4652-8931-9c5f8c4a3a44","metadata":{"id":"87ca000c-9188-4652-8931-9c5f8c4a3a44","executionInfo":{"status":"ok","timestamp":1742247036690,"user_tz":0,"elapsed":2631,"user":{"displayName":"Frank Conway","userId":"18233513917736815923"}}},"outputs":[],"source":["from transformers import ViTImageProcessor\n","import datasets\n","\n","# Load CIFAR-100 dataset\n","dataset = datasets.load_dataset(\"cifar100\")\n","img_class_labels = dataset['train'].features['fine_label'].names\n","# Rename column to labels for .to_tf_dataset later\n","dataset = dataset.rename_column(\"fine_label\", \"labels\")\n","\n","# Initialize the ViT image processor for 224x224 resizing and normalization\n","feature_extractor = ViTImageProcessor(\n","    size=224,\n","    do_resize=True,\n","    do_normalize=True,\n","    image_mean=[0.5, 0.5, 0.5],\n","    image_std=[0.5, 0.5, 0.5],\n",")\n","\n","# Create a preprocessing function\n","def preprocess_function(examples):\n","    images = examples[\"img\"]\n","    # Process images using ViTImageProcessor\n","    inputs = feature_extractor(images=images, return_tensors=\"tf\")\n","    inputs[\"labels\"] = examples[\"labels\"]\n","    return inputs\n","\n","# Apply preprocessing\n","processed_dataset = dataset.map(preprocess_function, batched=True)\n","\n","# Split into train/validation sets\n","test_size = 0.15\n","train_val_split = processed_dataset[\"train\"].train_test_split(test_size=test_size)\n","\n","# Create the final dataset dictionary\n","final_dataset = datasets.DatasetDict({\n","    \"train\": train_val_split[\"train\"],\n","    \"validation\": train_val_split[\"test\"],\n","    \"test\": processed_dataset[\"test\"]  # Use the original test set\n","})\n","\n","\n","\n","\n","\n","# # Number of classes in CIFAR-100\n","# num_classes = 100\n","\n","# # Function to convert sparse labels to one-hot encoded (categorical)\n","# def sparse_to_categorical(features):\n","#     features[\"categorical_labels\"] = tf.one_hot(features[\"labels\"], depth=num_classes)\n","#     return features\n","\n","# # Convert to TensorFlow datasets with categorical labels\n","# tf_train_dataset = final_dataset[\"train\"].to_tf_dataset(\n","#     columns=[\"pixel_values\", \"labels\"],\n","#     shuffle=True,\n","#     batch_size=16,\n","#     collate_fn=None,\n","# ).map(sparse_to_categorical)\n","\n","# tf_validation_dataset = final_dataset[\"validation\"].to_tf_dataset(\n","#     columns=[\"pixel_values\", \"labels\"],\n","#     shuffle=False,\n","#     batch_size=16,\n","#     collate_fn=None,\n","# ).map(sparse_to_categorical)\n","\n","# tf_test_dataset = final_dataset[\"test\"].to_tf_dataset(\n","#     columns=[\"pixel_values\", \"labels\"],\n","#     shuffle=False,\n","#     batch_size=16,\n","#     collate_fn=None,\n","# ).map(sparse_to_categorical)\n","\n","# Convert to TensorFlow datasets\n","tf_train_dataset = final_dataset[\"train\"].to_tf_dataset(\n","    columns=[\"pixel_values\", \"labels\"],\n","    shuffle=True,\n","    batch_size=16,\n","    collate_fn=None,\n",")\n","\n","tf_validation_dataset = final_dataset[\"validation\"].to_tf_dataset(\n","    columns=[\"pixel_values\", \"labels\"],\n","    shuffle=False,\n","    batch_size=16,\n","    collate_fn=None,\n",")\n","\n","tf_test_dataset = final_dataset[\"test\"].to_tf_dataset(\n","    columns=[\"pixel_values\", \"labels\"],\n","    shuffle=False,\n","    batch_size=16,\n","    collate_fn=None,\n",")"]},{"cell_type":"code","execution_count":null,"id":"83def2da-a7fc-4da7-b14b-bc17459819e7","metadata":{"id":"83def2da-a7fc-4da7-b14b-bc17459819e7","outputId":"ab59eead-64c0-4a1a-b3a8-d9f37394976b"},"outputs":[{"data":{"text/plain":["{'img': Image(mode=None, decode=True, id=None),\n"," 'labels': ClassLabel(names=['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'cra', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'], id=None),\n"," 'coarse_label': ClassLabel(names=['aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_and_vegetables', 'household_electrical_devices', 'household_furniture', 'insects', 'large_carnivores', 'large_man-made_outdoor_things', 'large_natural_outdoor_scenes', 'large_omnivores_and_herbivores', 'medium_mammals', 'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals', 'trees', 'vehicles_1', 'vehicles_2'], id=None),\n"," 'pixel_values': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None)}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["final_dataset[\"test\"].features"]},{"cell_type":"code","execution_count":7,"id":"8e14fd85-f21e-4bad-9256-c17b020d8ce3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e14fd85-f21e-4bad-9256-c17b020d8ce3","executionInfo":{"status":"ok","timestamp":1742246875682,"user_tz":0,"elapsed":1059,"user":{"displayName":"Frank Conway","userId":"18233513917736815923"}},"outputId":"a9f959ea-4c92-4d0c-b45a-cc15b76c63da"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFViTForImageClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["\n","model_name = \"google/vit-base-patch16-224-in21k\"\n","\n","id2label = {str(i): label for i, label in enumerate(img_class_labels)}\n","label2id = {v: k for k, v in id2label.items()}\n","\n","# load pre-trained ViT model\n","model = TFViTForImageClassification.from_pretrained(\n","    model_name,\n","    num_labels=len(img_class_labels),\n","    id2label=id2label,\n","    label2id=label2id,\n",")"]},{"cell_type":"code","execution_count":21,"id":"0eb56b5d-7141-4b99-9c38-c7a963ef2b8f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0eb56b5d-7141-4b99-9c38-c7a963ef2b8f","executionInfo":{"status":"ok","timestamp":1742255896121,"user_tz":0,"elapsed":5220516,"user":{"displayName":"Frank Conway","userId":"18233513917736815923"}},"outputId":"423016ad-499b-4769-a59e-896e52c03e60"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFViTForImageClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","2657/2657 [==============================] - 383s 129ms/step - loss: 2.0388 - accuracy: 0.8027 - val_loss: 0.6566 - val_accuracy: 0.8835\n","Epoch 2/10\n","2657/2657 [==============================] - 345s 130ms/step - loss: 0.3216 - accuracy: 0.9364 - val_loss: 0.3817 - val_accuracy: 0.9080\n","Epoch 3/10\n","2657/2657 [==============================] - 343s 129ms/step - loss: 0.1487 - accuracy: 0.9647 - val_loss: 0.3657 - val_accuracy: 0.9053\n","Epoch 4/10\n","2657/2657 [==============================] - 338s 127ms/step - loss: 0.0918 - accuracy: 0.9773 - val_loss: 0.4319 - val_accuracy: 0.8927\n","Epoch 5/10\n","2657/2657 [==============================] - 337s 127ms/step - loss: 0.0680 - accuracy: 0.9823 - val_loss: 0.4188 - val_accuracy: 0.9003\n","Epoch 6/10\n","2657/2657 [==============================] - 335s 126ms/step - loss: 0.0522 - accuracy: 0.9858 - val_loss: 0.4071 - val_accuracy: 0.9013\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFViTForImageClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","2657/2657 [==============================] - 380s 128ms/step - loss: 3.4746 - accuracy: 0.7039 - val_loss: 2.3883 - val_accuracy: 0.8705\n","Epoch 2/10\n","2657/2657 [==============================] - 348s 131ms/step - loss: 1.5662 - accuracy: 0.9068 - val_loss: 1.0317 - val_accuracy: 0.9003\n","Epoch 3/10\n","2657/2657 [==============================] - 346s 130ms/step - loss: 0.5819 - accuracy: 0.9478 - val_loss: 0.5395 - val_accuracy: 0.9079\n","Epoch 4/10\n","2657/2657 [==============================] - 345s 130ms/step - loss: 0.2412 - accuracy: 0.9687 - val_loss: 0.4162 - val_accuracy: 0.9068\n","Epoch 5/10\n","2657/2657 [==============================] - 346s 130ms/step - loss: 0.1258 - accuracy: 0.9790 - val_loss: 0.3894 - val_accuracy: 0.9083\n","Epoch 6/10\n","2657/2657 [==============================] - 348s 131ms/step - loss: 0.0755 - accuracy: 0.9859 - val_loss: 0.3787 - val_accuracy: 0.9093\n","Epoch 7/10\n","2657/2657 [==============================] - 341s 128ms/step - loss: 0.0515 - accuracy: 0.9897 - val_loss: 0.3941 - val_accuracy: 0.9095\n","Epoch 8/10\n","2657/2657 [==============================] - 342s 129ms/step - loss: 0.0391 - accuracy: 0.9915 - val_loss: 0.4048 - val_accuracy: 0.9104\n","Epoch 9/10\n","2657/2657 [==============================] - 341s 128ms/step - loss: 0.0290 - accuracy: 0.9938 - val_loss: 0.4162 - val_accuracy: 0.9105\n"]}],"source":["import pandas as pd\n","\n","lr = [3e-5, 1e-5]\n","\n","\n","for LR in lr:\n","\n","    model_name = \"ViT_\"+str(LR)\n","\n","    model_name = \"google/vit-base-patch16-224-in21k\"\n","\n","    id2label = {str(i): label for i, label in enumerate(img_class_labels)}\n","    label2id = {v: k for k, v in id2label.items()}\n","\n","\n","    # load pre-trained ViT model\n","    model = TFViTForImageClassification.from_pretrained(\n","        model_name,\n","        num_labels=len(img_class_labels),\n","        id2label=id2label,\n","        label2id=label2id,\n","    )\n","\n","    # Define optimizer and loss\n","    optimizer = tf.keras.optimizers.AdamW(learning_rate=LR)\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint(path+model_name+\".keras\", save_best_only=True, monitor='val_loss')\n","\n","    # Compile the model\n","    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n","\n","    # Train the model\n","    history = model.fit(\n","        tf_train_dataset,\n","        validation_data=tf_validation_dataset,\n","        epochs=10,\n","        callbacks=[early_stopping, checkpoint]\n","    )\n","\n","    # convert history to DataFrame\n","    history_df = pd.DataFrame(history.history)\n","\n","    history_df.to_csv(path+model_name+\".csv\", index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"5c3e5275-2482-48bc-a49d-85110da152ad","metadata":{"id":"5c3e5275-2482-48bc-a49d-85110da152ad","outputId":"37b98f69-9aec-4703-cafe-2b765bffc098"},"outputs":[{"ename":"ValueError","evalue":"Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x16a340d00>","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     17\u001b[0m     tf_train_dataset,\n\u001b[1;32m     18\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtf_validation_dataset,\n\u001b[1;32m     19\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1563\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[0;32m-> 1563\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1575\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   1576\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1583\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tf_keras/src/optimizers/__init__.py:335\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get(\n\u001b[1;32m    331\u001b[0m         config,\n\u001b[1;32m    332\u001b[0m         use_legacy_optimizer\u001b[38;5;241m=\u001b[39muse_legacy_optimizer,\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x16a340d00>"]}],"source":["# Import necessary metrics modules\n","from tensorflow.keras.metrics import Precision, Recall, F1Score\n","\n","# Define optimizer and loss\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","model.compile(\n","    optimizer=optimizer,\n","    loss=loss,\n","    metrics=[\n","        \"accuracy\"]\n",")\n","\n","# Train the model\n","history = model.fit(\n","    tf_train_dataset,\n","    validation_data=tf_validation_dataset,\n","    epochs=3\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"7e3d8344-60cf-45fa-85b0-94c2a984155f","metadata":{"id":"7e3d8344-60cf-45fa-85b0-94c2a984155f","outputId":"bbe752f0-eff1-4bab-f50b-96b6ec5104ff"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"]},{"ename":"ValueError","evalue":"Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x174793280>","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     precision, recall \u001b[38;5;241m=\u001b[39m precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m ((precision \u001b[38;5;241m*\u001b[39m recall) \u001b[38;5;241m/\u001b[39m (precision \u001b[38;5;241m+\u001b[39m recall \u001b[38;5;241m+\u001b[39m K\u001b[38;5;241m.\u001b[39mepsilon()))\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparseCategoricalCrossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     35\u001b[0m     tf_train_dataset,\n\u001b[1;32m     36\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtf_validation_dataset,\n\u001b[1;32m     37\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1563\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[0;32m-> 1563\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1575\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   1576\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1583\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tf_keras/src/optimizers/__init__.py:335\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get(\n\u001b[1;32m    331\u001b[0m         config,\n\u001b[1;32m    332\u001b[0m         use_legacy_optimizer\u001b[38;5;241m=\u001b[39muse_legacy_optimizer,\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x174793280>"]}],"source":["# Define optimizer and loss\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","from tensorflow.keras import backend as K\n","# Compile the model\n","# model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\",tf.keras.metrics.Precision(name='precision') ])\n","\n","def f1(y_true, y_pred):\n","    def recall_m(y_true, y_pred):\n","        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = TP / (Positives + K.epsilon())\n","        return recall\n","\n","    def precision_m(y_true, y_pred):\n","        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = TP / (Pred_Positives + K.epsilon())\n","        return precision\n","\n","    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n","    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n","\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[\"accuracy\", f1]\n",")\n","\n","\n","\n","# Train the model\n","history = model.fit(\n","    tf_train_dataset,\n","    validation_data=tf_validation_dataset,\n","    epochs=3\n",")"]},{"cell_type":"code","execution_count":null,"id":"8d784486-1221-4e9a-b8c9-84480ca89c1e","metadata":{"id":"8d784486-1221-4e9a-b8c9-84480ca89c1e","outputId":"7ae83228-0a18-487d-a597-fd09bbcf951b"},"outputs":[{"ename":"AttributeError","evalue":"'Variable' object has no attribute '_distribute_strategy'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# For precision and recall with sparse labels, convert predictions to classes first\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPrecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     23\u001b[0m     tf_train_dataset,\n\u001b[1;32m     24\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtf_validation_dataset,\n\u001b[1;32m     25\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1563\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[0;32m-> 1563\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1575\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m   1576\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1583\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4021\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended.variable_created_in_scope\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m   4020\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvariable_created_in_scope\u001b[39m(\u001b[38;5;28mself\u001b[39m, v):\n\u001b[0;32m-> 4021\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribute_strategy\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute '_distribute_strategy'"]}],"source":["# Import necessary metrics modules\n","from tensorflow.keras.metrics import Precision, Recall\n","import tensorflow as tf\n","\n","# Define optimizer and loss\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# Compile the model\n","model.compile(\n","    optimizer=optimizer,\n","    loss=loss,\n","    metrics=[\n","        \"accuracy\",\n","        # For precision and recall with sparse labels, convert predictions to classes first\n","        tf.keras.metrics.Precision(name='precision', top_k=1, class_id=None),\n","        tf.keras.metrics.Recall(name='recall', top_k=1, class_id=None)\n","    ]\n",")\n","\n","# Train the model\n","history = model.fit(\n","    tf_train_dataset,\n","    validation_data=tf_validation_dataset,\n","    epochs=3\n",")"]},{"cell_type":"code","execution_count":null,"id":"822ed994-ce6b-4c76-8940-871a792cf5f8","metadata":{"id":"822ed994-ce6b-4c76-8940-871a792cf5f8"},"outputs":[],"source":["dataset = datasets.load_dataset(\"cifar100\")\n","\n","img_class_labels = dataset['train'].features['fine_label'].names\n","# img_class_labels\n","# we are also renaming our label col to labels to use `.to_tf_dataset` later\n","dataset = dataset.rename_column(\"fine_label\", \"labels\")\n","\n","# Create a preprocessing function\n","def preprocess_function(examples):\n","    images = examples[\"img\"]\n","    inputs = feature_extractor(images=images, return_tensors=\"tf\")\n","    inputs[\"labels\"] = examples[\"labels\"]\n","    return inputs\n","\n","# Apply preprocessing\n","processed_dataset = dataset.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":null,"id":"189da8a7-81df-4117-875d-eafff82da245","metadata":{"id":"189da8a7-81df-4117-875d-eafff82da245"},"outputs":[],"source":["processed_dataset"]},{"cell_type":"code","execution_count":null,"id":"e72d74ed-fc0f-4ccf-b078-a9ebb199ad35","metadata":{"id":"e72d74ed-fc0f-4ccf-b078-a9ebb199ad35"},"outputs":[],"source":["tf_test_dataset = processed_dataset[\"test\"].to_tf_dataset(\n","    columns=[\"pixel_values\", \"labels\"],\n","    shuffle=True,\n","    batch_size=16,\n","    collate_fn=None,\n",")"]},{"cell_type":"code","execution_count":null,"id":"89ed65a7-f5bd-4078-9213-96422c785181","metadata":{"id":"89ed65a7-f5bd-4078-9213-96422c785181"},"outputs":[],"source":["model.evaluate(tf_test_dataset)"]},{"cell_type":"code","execution_count":null,"id":"97cbedf8-0657-449f-bb82-e223531d6642","metadata":{"id":"97cbedf8-0657-449f-bb82-e223531d6642","outputId":"4c3032f3-f988-40bc-8a04-b5bed60871a0"},"outputs":[{"data":{"text/plain":["'/Users/frankconway/Library/CloudStorage/OneDrive-Personal/Strathclyde/Strathclyde/Year5/EE992/Project/Vit'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10 (tensorflow)","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"28d48cdce69e4926886646d2e6c704b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c158ec86d6b4008ae170ca2967a4829","IPY_MODEL_0f55b2b6bc3c4763a34db2100e2317ec","IPY_MODEL_6f739c947f4b46e6bbcb1bf3da642447"],"layout":"IPY_MODEL_b7940900916d45f59fb480b4ee16c524"}},"5c158ec86d6b4008ae170ca2967a4829":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd0c6bacc163475eaf42896602590ff7","placeholder":"​","style":"IPY_MODEL_4862207b785a45b688495001752145c6","value":"preprocessor_config.json: 100%"}},"0f55b2b6bc3c4763a34db2100e2317ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f2b83dff1a14f1899c066d417b5c3e8","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_383868970b9b44de821c1c6f28f71bd2","value":160}},"6f739c947f4b46e6bbcb1bf3da642447":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bbc4e7b7b9b4cef96568eeb25f9e003","placeholder":"​","style":"IPY_MODEL_eaf091a26b7d41ec9e9ccc21b485357d","value":" 160/160 [00:00&lt;00:00, 17.4kB/s]"}},"b7940900916d45f59fb480b4ee16c524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd0c6bacc163475eaf42896602590ff7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4862207b785a45b688495001752145c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f2b83dff1a14f1899c066d417b5c3e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"383868970b9b44de821c1c6f28f71bd2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bbc4e7b7b9b4cef96568eeb25f9e003":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaf091a26b7d41ec9e9ccc21b485357d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff40db26d2fe49c6abe1c112f7ed4867":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b0e000909484c3da6f9e2775e66831c","IPY_MODEL_1cc3449771c540eab0e3a8f1833144ec","IPY_MODEL_ae59fcd287f14348b7cbac41e8f74fdf"],"layout":"IPY_MODEL_27e1d19e670e462c989465fa612a5a3c"}},"7b0e000909484c3da6f9e2775e66831c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_027427d6290c4fc180123ccb8f170090","placeholder":"​","style":"IPY_MODEL_1a722597e3dd4382b2539cd68ca12b97","value":"config.json: 100%"}},"1cc3449771c540eab0e3a8f1833144ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_797e675107d74ed7a11eb0c1d429e48c","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1fd872b209f34f4bb66480c20f3225d4","value":502}},"ae59fcd287f14348b7cbac41e8f74fdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dc8cca78e7c4f4b914cf78f757ab889","placeholder":"​","style":"IPY_MODEL_2f87ca7f64854cc19a2fd13e8920d3b2","value":" 502/502 [00:00&lt;00:00, 58.9kB/s]"}},"27e1d19e670e462c989465fa612a5a3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"027427d6290c4fc180123ccb8f170090":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a722597e3dd4382b2539cd68ca12b97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"797e675107d74ed7a11eb0c1d429e48c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fd872b209f34f4bb66480c20f3225d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3dc8cca78e7c4f4b914cf78f757ab889":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f87ca7f64854cc19a2fd13e8920d3b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24b07896645943c79d433b5d07ff252d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_938ec031bab548e88dff2076fcbf9740","IPY_MODEL_26959b84c1a64450ad97fb9fa2dcbb60","IPY_MODEL_efb6a193137d425a98b653b39af902b3"],"layout":"IPY_MODEL_1fd2d69786d441479b3de47a43125291"}},"938ec031bab548e88dff2076fcbf9740":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_755f07bbfff7431bb9e69c57a90396c3","placeholder":"​","style":"IPY_MODEL_a8a9ca0e0374407c87c23b6312f42e83","value":"model.safetensors: 100%"}},"26959b84c1a64450ad97fb9fa2dcbb60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75a4e280162447be837954b0aeef6750","max":345579424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7095f0881c604498aeb4f13b0c5bdd35","value":345579424}},"efb6a193137d425a98b653b39af902b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eef9813ac5c5413c95096adee1a56a6c","placeholder":"​","style":"IPY_MODEL_bd307ed83cab463d9d5930b22b00c4e3","value":" 346M/346M [00:01&lt;00:00, 226MB/s]"}},"1fd2d69786d441479b3de47a43125291":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"755f07bbfff7431bb9e69c57a90396c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8a9ca0e0374407c87c23b6312f42e83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75a4e280162447be837954b0aeef6750":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7095f0881c604498aeb4f13b0c5bdd35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eef9813ac5c5413c95096adee1a56a6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd307ed83cab463d9d5930b22b00c4e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}