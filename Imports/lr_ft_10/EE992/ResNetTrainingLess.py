# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14sZNLxYdKAbqmDqeqmJ0CYKklIq0ZpJe
"""

import tensorflow as tf
from tensorflow.keras.datasets import cifar100
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

#CIFAR-100 dataset
(x_train, y_train), (x_test, y_test) = cifar100.load_data()

x_train, x_val, y_train, y_val = train_test_split(
    x_train, y_train, test_size=0.2, random_state=42)

np.savez_compressed("test_dataset.npz", images=x_test, labels=y_test)

#convert pixel values to float32 and normalize to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_val = x_val.astype('float32') / 255.0

y_train = tf.keras.utils.to_categorical(y_train, num_classes=100)
y_val = tf.keras.utils.to_categorical(y_val, num_classes=100)


#batch size
BATCH_SIZE = 8

#resizing images
def resize_images(image, label):
    image = tf.image.resize(image, [224, 224])
    return image, label

#augmenting images
def augment(image, label):
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, max_delta=0.2)
    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
    return image, label

#training dataset pipeline
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.map(resize_images, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

#test dataset pipeline (only resizing, no augmentation)
test_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
test_dataset = test_dataset.map(resize_images, num_parallel_calls=tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

#callbacks
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = tf.keras.callbacks.ModelCheckpoint("best_model.keras", save_best_only=True, monitor='val_loss')

# #metrics
# from tensorflow.keras.metrics import Precision, Recall, Accuracy, SparseCategoricalAccuracy

# METRICS = [SparseCategoricalAccuracy(), Precision(), Recall()]

from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall, AUC

METRICS = [
    CategoricalAccuracy(name='accuracy'),
    Precision(name='precision'),
    Recall(name='recall'),
    AUC(name='auc')
]

def train(base_model, train_dataset, val_dataset, DROPOUT = 0.5, LR = 1e-3, EPOCHS=30):

    #freeze the base model
    base_model.trainable = False

    #add layers
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(1024, activation='relu'),
        layers.Dense(256, activation='relu'),
        layers.Dropout(DROPOUT),
        layers.Dense(100, activation='softmax')  # 100 classes
    ])

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR),
                  loss='categorical_crossentropy',
                  metrics=METRICS)

    #model.summary()

    model_name = base_model.name + "_" + str(DROPOUT) + "_" + str(LR) + "_"

    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_name+"tl.keras", save_best_only=True, monitor='val_loss')
    #csv_logger = tf.keras.callbacks.CSVLogger(model_name+"tl.csv")

    history = model.fit(train_dataset, epochs=EPOCHS, validation_data=(val_dataset),callbacks=[early_stopping, checkpoint], verbose=0)

    # Convert history to DataFrame
    history_df = pd.DataFrame(history.history)

    # Save to CSV
    history_df.to_csv(model_name+"tl.csv", index=False)

    #load the saved best model
    fine_tune_model = model#tf.keras.models.load_model(model_name+"tl.keras")
    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_name+"ft.keras", save_best_only=True, monitor='val_loss')
    #csv_logger = tf.keras.callbacks.CSVLogger(model_name+"ft.csv")


    #unfreeze some layers for fine-tuning
    for layer in fine_tune_model.layers[:-1]:  # Keep last layer frozen
        layer.trainable = True

    fine_tune_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR/10.0),
                            loss='categorical_crossentropy',
                            metrics=METRICS)

    #fine_tune_model.summary()

    fine_tune_history = fine_tune_model.fit(train_dataset, epochs=EPOCHS, validation_data=(val_dataset),callbacks=[early_stopping, checkpoint], verbose=0)

     # Convert history to DataFrame
    history_df = pd.DataFrame(fine_tune_history.history)

     # Save to CSV
    history_df.to_csv(model_name+"ft.csv", index=False)


bases = [tf.keras.applications.ResNet50V2(input_shape=(224, 224, 3),
                                            include_top=False,
                                            weights='imagenet'),
         tf.keras.applications.ResNet152V2(input_shape=(224, 224, 3),
                                            include_top=False,
                                            weights='imagenet')]

for MODEL in bases:
  for LR in [1e-2, 1e-3, 1e-5]:
      for DROPOUT in [0.2, 0.4, 0.6]:
          train(MODEL, train_dataset=train_dataset, val_dataset=test_dataset, DROPOUT=DROPOUT, LR=LR, EPOCHS=100)

